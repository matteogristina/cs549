{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 8: Optimization of Deep Neural Networks\n",
    "    \n",
    "    \n",
    "**Total points: 15**\n",
    "\n",
    "In this assignment, you will implement a multiple layer feed-forward neural network for a multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Build a deep neural network model\n",
    "**Points: 3**\n",
    "\n",
    "Implement the `NeuralNetModel1` class. The model takes a $28\\times 28$ grey-scale image as input, and pass it through a deep neural network.\n",
    "\n",
    "The network has 2 hidden layers and 1 output layers, whose sizes are: 512 -> 512 -> 10. That is, the number of output classes is 10. The activation function for each hidden layer is `ReLU`.\n",
    "\n",
    "The input image is first passed through a `nn.Flatten()` layer so that a 2D tensor becomes 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetModel1, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.flatten = nn.Flatten() # Use nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(784, 512), # Input size is 28*28\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.Linear(512, 512), # 512 -> 512\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.Linear(512, 10), # 512 -> 10\n",
    "        )\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START YOUR CODE ###\n",
    "        x = self.flatten(x) # Call self.flatten()\n",
    "        logits = self.linear_relu_stack(x) # Call self.linear_relu_stack()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([5, 28, 28])\n",
      "output size: torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "sample_input = torch.randn(5, 28, 28)\n",
    "print('input size:', sample_input.size())\n",
    "\n",
    "model1 = NeuralNetModel1()\n",
    "with torch.no_grad():\n",
    "    output = model1(sample_input)\n",
    "print('output size:', output.size())\n",
    "#print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "input size: torch.Size([5, 28, 28])\\\n",
    "output size: torch.Size([5, 10])\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2: Use dataloader\n",
    "**Points: 1**\n",
    "\n",
    "Download the FashionMNIST dataset provided by PyTorch to the folder \"data\", which takes some time for the first time execution.\n",
    "Use the `DataLoader` module to wrap the loaded training and test data. Specify the `batch_size` correctly for both training and test dataloader.\n",
    "\n",
    "See <https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True, # True\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # False\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "### START YOUR CODE ###\n",
    "train_loader = DataLoader(training_data, batch_size=64) # Specify data source and batch size correctly\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 60000\n",
      "Testing data size: 10000\n",
      "X size: torch.Size([64, 1, 28, 28])\n",
      "y size: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "print('Training data size:', len(training_data))\n",
    "print('Testing data size:', len(test_data))\n",
    "\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print('X size:', X.size())\n",
    "    print('y size:', y.size())\n",
    "   # print(y)\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "Training data size: 60000\\\n",
    "Testing data size: 10000\\\n",
    "X size: torch.Size([64, 1, 28, 28])\\\n",
    "y size: torch.Size([64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Define loss and optimizer\n",
    "**Points: 1**\n",
    "\n",
    "Use `nn.CrossEntropyLoss()` as the loss function, and use `torch.optim.SGD()` as the optimizer. Specify the arguments for `SGD()`, including the learning rate correctly.\n",
    "\n",
    "See <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html> and <https://pytorch.org/docs/stable/optim.html> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "### START YOUR CODE ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_sgd = torch.optim.SGD(model1.parameters(), lr = learning_rate)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "<class 'torch.optim.sgd.SGD'>\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "print(loss_fn)\n",
    "print(type(optimizer_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "CrossEntropyLoss()\n",
    "<class 'torch.optim.sgd.SGD'>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4: Implement train and test functions\n",
    "**Points: 6**\n",
    "\n",
    "Implement the code for training the model in `train()`. Implement the code for testing the model in `test()`. For the backpropagation step, you need to first zero out all gradients by calling `optimizer.zero_grad()` before carrying out `backward()` and `step()` to update parameters.\n",
    "\n",
    "In `test()`, you need to calculate the number of correct prediction in the current batch, and add it to the `correct` variable.\n",
    "Finally, you need to divide `correct` by the total number of test examples to obtain the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, verbose=True):\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        ### START YOUR CODE ###\n",
    "        pred = model1(X) # Get the prediction output from model\n",
    "        loss = loss_fn(pred, y) # compute loss by calling loss_fn()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Backpropagation\n",
    "        ### START YOUR CODE ###\n",
    "        optimizer.zero_grad() # zero_grad()\n",
    "        loss.backward() # backward()\n",
    "        optimizer.step() # step()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            current_step = i * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current_step:>5d}/{len(dataloader.dataset):>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        ### START YOUR CODE ###\n",
    "        pred = model1(X) # Similar to how it is computed in train()\n",
    "        #print(\"predsize: \", pred.size[0])\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item()\n",
    "        correct += (torch.eq((torch.argmax(pred, 1)), y).sum())  # Add the number of correct prediction in the current batch to `correct`\n",
    "        total += pred.size(0)\n",
    "        #pred.size(0)\n",
    "        #print(\"correct: \", torch.eq((torch.argmax(pred, 1)), y).sum())\n",
    "        #print(\"tot:\", total)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    ### START YOUR CODE ###\n",
    "    test_acc = correct / total # Use `correct` to compute accuracy\n",
    "    #print(\"total: \", correct)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, execute the following cell to start the training and testing loop. Make sure that the cell containing the loss function and optimizers has already been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296701  [    0/60000]\n",
      "loss: 2.284246  [ 6400/60000]\n",
      "loss: 2.266165  [12800/60000]\n",
      "loss: 2.270359  [19200/60000]\n",
      "loss: 2.252517  [25600/60000]\n",
      "loss: 2.231632  [32000/60000]\n",
      "loss: 2.239763  [38400/60000]\n",
      "loss: 2.206202  [44800/60000]\n",
      "loss: 2.202750  [51200/60000]\n",
      "loss: 2.174662  [57600/60000]\n",
      "total:  tensor(3999)\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.171465 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.178695  [    0/60000]\n",
      "loss: 2.166121  [ 6400/60000]\n",
      "loss: 2.112745  [12800/60000]\n",
      "loss: 2.130643  [19200/60000]\n",
      "loss: 2.084809  [25600/60000]\n",
      "loss: 2.037291  [32000/60000]\n",
      "loss: 2.053999  [38400/60000]\n",
      "loss: 1.979528  [44800/60000]\n",
      "loss: 1.982393  [51200/60000]\n",
      "loss: 1.910790  [57600/60000]\n",
      "total:  tensor(5863)\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.913629 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.944622  [    0/60000]\n",
      "loss: 1.911706  [ 6400/60000]\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetModel1() # Reset the model\n",
    "### START YOUR CODE ###\n",
    "learning_rate = 1e-3\n",
    "optimizer_sgd = torch.optim.SGD(model1.parameters(), lr = learning_rate) # Because the model1 is reset, the optimizer also needs redefined.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ### START YOUR CODE ###\n",
    "    train_loop(train_loader, model1, loss_fn, optimizer_sgd, verbose=True) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, model1, loss_fn)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**\n",
    "\n",
    "The test accuracy from the last epoch should be above 70%.\n",
    "\n",
    "---\n",
    "\n",
    "Next, train an ADAM optimizer. Note that the model needs be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model1 = NeuralNetModel1() # Reset the model\n",
    "\n",
    "### START YOUR CODE ###\n",
    "learning_rate = 1e-3\n",
    "optimizer_adam = torch.optim.Adam(model1.parameters(), lr = learning_rate)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ### START YOUR CODE ###\n",
    "    train_loop(train_loader, model1, loss_fn, optimizer_adam, verbose=True) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, model1, loss_fn)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "\n",
    "You can find that the training converges much faster using ADAM.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 5: Add batchnorm and dropout\n",
    "**Points: 4**\n",
    "\n",
    "Use `torch.nn.BatchNorm1d()` and `nn.Dropout()` after the ReLU activation of each hidden layer. `Batchnorm1d()` takes the size of previous activation as input. `Dropout()` takes the probability of dropout as input.\n",
    "\n",
    "For more information, see <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> and <https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel2(nn.Module):\n",
    "    def __init__(self, dropout = 0.1): # Note the additional dropout parameter here\n",
    "        \"\"\"\n",
    "        :param dropout: float, the probability of dropout\n",
    "        \"\"\"\n",
    "        super(NeuralNetModel2, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.BatchNorm1d(512), # Batchnorm\n",
    "            nn.Dropout(dropout), # Dropout, use the `dropout` parameter\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.BatchNorm1d(512), # Batchnorm\n",
    "            nn.Dropout(dropout), # Dropout, use the `dropout` parameter\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START YOUR CODE ###\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, test with different `dropout` rates, and observe how that affects the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296584  [    0/60000]\n",
      "loss: 2.300040  [ 6400/60000]\n",
      "loss: 2.294567  [12800/60000]\n",
      "loss: 2.295455  [19200/60000]\n",
      "loss: 2.312557  [25600/60000]\n",
      "loss: 2.288245  [32000/60000]\n",
      "loss: 2.303670  [38400/60000]\n",
      "loss: 2.295124  [44800/60000]\n",
      "loss: 2.293981  [51200/60000]\n",
      "loss: 2.299756  [57600/60000]\n",
      "total:  tensor(984)\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss: 2.300851 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.296584  [    0/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m### START YOUR CODE ###\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_loop(train_loader, model2, loss_fn, optimizer, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# Use verbose=False, if you want to see less information\u001b[39;00m\n\u001b[0;32m     12\u001b[0m test_loop(test_loader, model2, loss_fn)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m### END YOUR CODE ###\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer, verbose)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m### END YOUR CODE ###\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m### START YOUR CODE ###\u001b[39;00m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# zero_grad()\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# backward()\u001b[39;00m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# step()\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m### END YOUR CODE ###\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "model2 = NeuralNetModel2(0.1) # Call NeuralNetModel2() with the dropout value you want to try\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr = learning_rate) # You may try Adam/SGD optimizer\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ### START YOUR CODE ###\n",
    "    train_loop(train_loader, model2, loss_fn, optimizer, verbose=True) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, model2, loss_fn)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**\n",
    "\n",
    "In theory, you should see that the larger dropout rate you use, the lower test accuracy you will get, at the same epoch number.\n",
    "\n",
    "But the model trained with some dropout rate should generalize better to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
