{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q375Ssbh8ap0"
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 12: Transformer and Transformer-based Models (part 1)\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will: \n",
    "1) Implement the **multiple head attention** sub layer in a transformer encoder.\n",
    "2) Play with the transformer-based models provided in **transformers** for multiple natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GPDPovUW8ap4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwYXNhJe8ap6"
   },
   "source": [
    "## Task 1. Implement the multiple head attention sub layer\n",
    "**Points: 5**\n",
    "\n",
    "### 1.1 Initialize input data\n",
    "Step 1, generate some random input data in the shape of $\\text{n\\_inputs}\\times \\text{d\\_model}$. *Hint*: Use `np.random.rand()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2eobFj7i8ap6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # Do not remove this line\n",
    "\n",
    "d_model = 512\n",
    "n_inputs = 3\n",
    "\n",
    "### START YOUR CODE ###\n",
    "x = np.random.rand(n_inputs, d_model)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cnu5-K1L8ap7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\n",
      " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\n",
      " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\n",
      "x.shape: (3, 512)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('x:', x)\n",
    "print('x.shape:', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIXWoa1X8ap8"
   },
   "source": [
    "**Expected output**\\\n",
    "x: [[0.5488135  0.71518937 0.60276338 ... 0.44613551 0.10462789 0.34847599]\\\n",
    " [0.74009753 0.68051448 0.62238443 ... 0.6204999  0.63962224 0.9485403 ]\\\n",
    " [0.77827617 0.84834527 0.49041991 ... 0.07382628 0.49096639 0.7175595 ]]\\\n",
    "x.shape: (3, 512)\n",
    "\n",
    "---\n",
    "### 1.2 Create weight matrices for *query*, *key*, and *value*\n",
    "\n",
    "Step 2, create the weight matrices into the correct dimensions. \n",
    "\n",
    "Let's start with `W_query` and `Q`. *Hint*: We first initialize an empty tensor `W` in the dimension of `(d_model, d_k)`, using the `torch.empty()` function. Then we initialize it with `nn.init.xavier_uniform_()`.\n",
    "\n",
    "After `W_query` is initialized, we can get the query matrix `Q` with a multiplication between `x` and `W_query`. *Hint*: Use `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NYS70w2E8ap9"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 8\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty((d_model, d_k)) # Create an empty tensor W with the correct dimension.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W) # Randomly initialize the values in the tensor.\n",
    "W_query = W.data.numpy() # Copy out the numpy array\n",
    "\n",
    "### START YOUR CODE ###\n",
    "Q = np.matmul(x, W_query)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nR6gl2Wr8ap-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\n",
      "W_query.shape: (512, 64)\n",
      "Q[0, :5]: [-0.22772415  0.48167861  1.48693408 -1.00410576  0.19323685]\n",
      "Q.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('W_query[0,:5]:', W_query[0,:5])\n",
    "print('W_query.shape:', W_query.shape)\n",
    "print('Q[0, :5]:', Q[0,:5])\n",
    "print('Q.shape:', Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M_xE7qW8ap_"
   },
   "source": [
    "**Expected output**\\\n",
    "W_query[0,:5]: [-0.00076412  0.05475055 -0.0840017  -0.07511146 -0.03930965]\\\n",
    "W_query.shape: (512, 64)\\\n",
    "Q[0, :5]: [-0.22772415  0.48167861  1.48693408 -1.00410576  0.19323685]\\\n",
    "Q.shape: (3, 64)\n",
    "\n",
    "---\n",
    "Next, repeat for `W_key` & `K`, and `W_value` & `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3gxV7dCf8aqA"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1) # Do not remove this line\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty((d_model, d_k))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_key = W.data.numpy()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "K = np.matmul(x, W_key)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BFjO91aB8aqA"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2) # Do not remove this line\n",
    "\n",
    "### START YOUR CODE ###\n",
    "W = torch.empty((d_model, d_k))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "nn.init.xavier_uniform_(W)\n",
    "W_value = W.data.numpy()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "V = np.matmul(x, W_value)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6tF9bStn8aqB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K[0,:5] [ 0.2283654  -0.65482728 -0.07202067  0.49886374  0.57045028]\n",
      "K.shape (3, 64)\n",
      "V[0,:5] [-0.44997754  0.92097362 -0.76932045  0.03289757 -0.49462588]\n",
      "V.shape (3, 64)\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('K[0,:5]', K[0,:5])\n",
    "print('K.shape', K.shape)\n",
    "print('V[0,:5]', V[0,:5])\n",
    "print('V.shape', V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5_Rm_cD8aqC"
   },
   "source": [
    "**Expected output**\\\n",
    "K[0,:5] [ 0.2283654  -0.65482728 -0.07202067  0.49886374  0.57045028]\\\n",
    "K.shape (3, 64)\\\n",
    "V[0,:5] [-0.44997754  0.92097362 -0.76932045  0.03289757 -0.49462588]\\\n",
    "V.shape (3, 64)\n",
    "\n",
    "---\n",
    "### 1.3 Compute attention scores and weighted output\n",
    "\n",
    "Step 3, compute the attension scores using the matrices `Q` and `K`, following the equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Attention(Q, K, V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}})V\n",
    "\\end{equation}\n",
    "\n",
    "in which $\\sqrt{d_k}$ is for normalization purpose.\n",
    "\n",
    "*Hint*: You should first compute `attn_scores`, which is the unnormalized score. Then you can apply the `softmax()` function imported from `scipy` to calculate the normalized scores. Note that you need to specify the `axis` argument correctly when you call `softmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2yBs2CjJ8aqC"
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "attn_scores = ((Q @ K.transpose()) / np.sqrt(d_k))\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "### START YOUR CODE ###\n",
    "attn_scores_norm = softmax(attn_scores, axis = 1)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PDbykYOp8aqC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores.shape: (3, 3)\n",
      "Unnormalized attn_scores: [[-0.75497307 -0.97036233 -0.85112729]\n",
      " [ 0.23777018 -0.70730381 -0.37639239]\n",
      " [ 0.21608578 -0.73905372 -0.89881112]]\n",
      "Normalized atten_scores: [[0.36838498 0.29700212 0.33461289]\n",
      " [0.51820328 0.20140013 0.2803966 ]\n",
      " [0.58387084 0.22464925 0.19147991]]\n"
     ]
    }
   ],
   "source": [
    "# Do not change the code in this cell\n",
    "print('attn_scores.shape:', attn_scores.shape)\n",
    "print('Unnormalized attn_scores:', attn_scores)\n",
    "print('Normalized atten_scores:', attn_scores_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ-NXVAX8aqD"
   },
   "source": [
    "**Expected output**\\\n",
    "attn_scores.shape: (3, 3)\\\n",
    "Unnormalized attn_scores: [[-0.75497307 -0.97036233 -0.85112729]\\\n",
    " [ 0.23777018 -0.70730381 -0.37639239]\\\n",
    " [ 0.21608578 -0.73905372 -0.89881112]]\\\n",
    "Normalized atten_scores: [[0.36838498 0.29700212 0.33461289]\\\n",
    " [0.51820328 0.20140013 0.2803966 ]\\\n",
    " [0.58387084 0.22464925 0.19147991]]\\\n",
    "\n",
    "---\n",
    "\n",
    "Step 4, finally, compute the output as the weighted sum of value (`V`), using the above computed `attn_scores_norm` as the weight.\n",
    "\n",
    "*Hint*: `attn_scores_norm[0,:]` is the weight for the first output `weighted_output[0,:]`, \\\n",
    "so the computation is:\\\n",
    "`weighted_output[0,:] = attn_scores_norm[0,0] * V[0,:] + attn_scores_norm[0,1] * V[1,:] + attn_scores_norm[0,2] * V[2,:]`. \\\n",
    "But you can achieve this with one line code using `np.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3qajj14q8aqD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_output[0,:5]: [-0.37040031  0.493314   -0.78595572  0.09711595 -0.33551551]\n",
      "weighted_output.shape: (3, 64)\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "weighted_output = np.matmul(attn_scores_norm, V)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('weighted_output[0,:5]:', weighted_output[0,:5])\n",
    "print('weighted_output.shape:', weighted_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMcnfq6j8aqE"
   },
   "source": [
    "**Expected output**\\\n",
    "weighted_output[0,:5]: [-0.37040031  0.493314   -0.78595572  0.09711595 -0.33551551]\\\n",
    "weighted_output.shape: (3, 64)\n",
    "\n",
    "---\n",
    "**Congratulation!** You have finished Task 1, and now you know how to implement the self-attention module, which is the core technique of Transformer."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
