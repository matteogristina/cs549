{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PjpuxXR8t7m"
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 12: Transformer and Transformer-based Models (part 2)\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will: \n",
    "1) Implement the **multiple head attention** sub layer in a transformer encoder.\n",
    "\n",
    "2) Play with the transformer-based models provided in **transformers** for multiple natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "07iMXEUK8t7r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA60pT4u8t7t"
   },
   "source": [
    "## Task 2. Play with transformer-based models\n",
    "**Points: 5**\n",
    "\n",
    "### 2.1 Installation\n",
    "Install the *transformers* package with the following command:\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "After it is done, you can load some pretrained BERT models and tokenizers like this (you can ignore the warnings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XHX5F-6t8t7u"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvvIhQyK8t7u"
   },
   "source": [
    "### 2.2 Tokenizing inputs\n",
    "\n",
    "Run the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AIaKtTur8t7v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "torch.Size([1, 275])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic\n",
    "resources for human beings. From a thermodynamic point of view, any energy conversion systems\n",
    "that receive energy from the sun and/or dissipate energy to the universe are heat engines with\n",
    "photons as the \"working fluid\" and can be analyzed using the concept of entropy. While entropy\n",
    "analysis provides a particularly convenient way to understand the efficiency limits, it is typically\n",
    "taught in the context of thermodynamic cycles among quasi-equilibrium states and its\n",
    "generalization to solar energy conversion systems running in a continuous and non-equilibrium\n",
    "fashion is not straightforward. In this educational article, we present a few examples to illustrate\n",
    "how the concept of photon entropy, combined with the radiative transfer equation, can be used to\n",
    "analyze the local entropy generation processes and the efficiency limits of different solar energy\n",
    "conversion systems. We provide explicit calculations for the local and total entropy generation\n",
    "rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily\n",
    "reproduced by students. We further discuss the connection between the entropy generation and the\n",
    "device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical\n",
    "efficiency limit.\"\"\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(len(text.split()))\n",
    "print(encoded_input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDCvvBEW8t7w"
   },
   "source": [
    "Can you explain why the `encoded_input` has more elements than the actual number of words in `text`?\\\n",
    "(**Points: 1**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tQjqNrgl8t7x"
   },
   "outputs": [],
   "source": [
    "### Write your answer within the quotes ###\n",
    "answer = \"\"\"\n",
    "    The encoded input has more elements than the actual number of words becuase the input is seperated into\n",
    "    useful chunks. For example, the word \"hotness\" is split into 2 \"words\", hot and ness. To the model, hot and \n",
    "    ness' can be parsed seperately, or have their information be combined to achiece the meaning of hotness.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGFYF1Mg8t7y"
   },
   "source": [
    "*NOTE*: there is no expected output for this question.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Output word vectors from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Erf0c1KW8t7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 275, 768])\n"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L9vmOBu8t7z"
   },
   "source": [
    "With the following code, you can find the corresponding token of each integer id in `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "59jfY6DG8t7z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 2980, 2791, 1997, 1996, 3103, 1998, 1996, 3147]\n",
      "['[CLS]', 'the', 'hot', '##ness', 'of', 'the', 'sun', 'and', 'the', 'cold']\n"
     ]
    }
   ],
   "source": [
    "input_ids_pt = encoded_input['input_ids']\n",
    "input_ids_list = input_ids_pt.tolist()[0]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "\n",
    "print(input_ids_list[:10])\n",
    "print(input_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1HiiCNT8t70"
   },
   "source": [
    "Can you find the output vector**s** among `last_hidden_state` that correpond to the input word \"entropy\"?\\\n",
    "Do they have the same values?\\\n",
    "**(Points: 1)**\n",
    "\n",
    "*Hint*: You can use a `if` statement to check if the current token is the word \"entropy\", and if so, you can append it to `vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-AMwmmSr8t70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"entropy\": 6\n",
      "Do they have the same value? [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i, token in enumerate(input_tokens):\n",
    "    ### START YOUR CODE ###\n",
    "    if (input_tokens[i] == \"entropy\"):\n",
    "            vectors.append(i)\n",
    "            \n",
    "vectors = torch.tensor(vectors)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Do not change the code below\n",
    "print('Number of \"entropy\":', len(vectors))\n",
    "\n",
    "matches = [torch.allclose(vectors[i], vectors[i+1]) for i in range(len(vectors)-1)]\n",
    "print(f'Do they have the same value? {matches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySpNfE9I8t71"
   },
   "source": [
    "**Expected output:** \\\n",
    "Number of \"entropy\": 6\\\n",
    "Do they have the same value? [False, False, False, False, False]\n",
    "\n",
    "---\n",
    "### 2.4 Sentence vectors from BERT\n",
    "\n",
    "We can obtain the output vectors for a batch of sentences.\n",
    "\n",
    "First, we need to break the text into a list of sentences, using a simple end-of-sentence str '.' as a separater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G9SbxEgO8t71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting in 6 sentences:\n",
      "['The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic resources for human beings.', 'From a thermodynamic point of view, any energy conversion systems that receive energy from the sun and/or dissipate energy to the universe are heat engines with photons as the \"working fluid\" and can be analyzed using the concept of entropy.', 'While entropy analysis provides a particularly convenient way to understand the efficiency limits, it is typically taught in the context of thermodynamic cycles among quasi-equilibrium states and its generalization to solar energy conversion systems running in a continuous and non-equilibrium fashion is not straightforward.', 'In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.', 'We provide explicit calculations for the local and total entropy generation rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily reproduced by students.', 'We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.replace('\\n', ' ').split('.')\n",
    "sentences = [s.strip() + '.' for s in sentences if len(s.strip())>0] # Some cleaning work\n",
    "\n",
    "print(f'Resulting in {len(sentences)} sentences:')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBT2qJOw8t71"
   },
   "source": [
    "Now, let's use tokenizer on this batch of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "O3ZqqmVg8t72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57])\n",
      "tensor([  101,  1996,  2980,  2791,  1997,  1996,  3103,  1998,  1996,  3147,\n",
      "         2791,  1997,  1996,  6058,  2686,  2024,  1999, 10288, 13821,  3775,\n",
      "         3468,  1996, 10867,  7716, 18279,  7712,  4219,  2005,  2529,  9552,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(encoded_sentences['input_ids'].shape)\n",
    "print(encoded_sentences['input_ids'][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYA7h9OO8t72"
   },
   "source": [
    "You can find that shorter sentences are padded with a special id `0`.\n",
    "\n",
    "Next, we can obtain the output tensors for all input sentences, also in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ti6zE5Ng8t72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoded_sentences)\n",
    "\n",
    "print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcLgx9xI8t72"
   },
   "source": [
    "Note that the first dimension of `outputs['last_hidden_state']` is batch size. So the output tensor for the 1st sentence is `outputs['last_hidden_state'][0]`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kV2WoMCU8t73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs['last_hidden_state'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3e9KZx28t73"
   },
   "source": [
    "For each output tensor, the first 768-dim vector (at position 0) always corresponds to the special input token `[CLS]`. We can use this vector to represent the meaning of the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cim3ChPi8t73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "CLS_vec = outputs['last_hidden_state'][0][0]\n",
    "print(CLS_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWopMVKR8t74"
   },
   "source": [
    "Now, it is your task to compute the cosine similarities between each pair of the 6 sentences, and find the pair that has the closest meanings.\\\n",
    "**(Points: 3)**\n",
    "\n",
    "*Hint*: You can use the `cosine_similarity()` function imported at the beginning, which takes input two tensors and returns the similarity score in a tensor. So you will need to append a `.item()` to retrieve the numeric value from the returned tensor. You also need to specify the argument `dim=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "v5OQcKmD8t74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <-> 1: 0.8591638207435608\n",
      "0 <-> 2: 0.777198314666748\n",
      "0 <-> 3: 0.7985225319862366\n",
      "0 <-> 4: 0.7754685878753662\n",
      "0 <-> 5: 0.805216372013092\n",
      "1 <-> 2: 0.876341700553894\n",
      "1 <-> 3: 0.8321617841720581\n",
      "1 <-> 4: 0.8238447904586792\n",
      "1 <-> 5: 0.8492751121520996\n",
      "2 <-> 3: 0.8241375684738159\n",
      "2 <-> 4: 0.8598626255989075\n",
      "2 <-> 5: 0.8579833507537842\n",
      "3 <-> 4: 0.9018083214759827\n",
      "3 <-> 5: 0.929144024848938\n",
      "4 <-> 5: 0.9185265898704529\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(i+1, 6):\n",
    "        ### START YOUR CODE ###\n",
    "        vec_i = outputs['last_hidden_state'][i][0]\n",
    "        vec_j = outputs['last_hidden_state'][j][0]\n",
    "        sim = cosine_similarity(vec_i, vec_j, dim = 0)\n",
    "        # Hint: when you call cosine_similarity() function,\n",
    "        # remember to specify dim=0. Also, you need append .item() at\n",
    "        # the end to obtain a number instead of a tensor.\n",
    "        \n",
    "        \n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "        print(f'{i} <-> {j}: {sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf5xshFV8t74"
   },
   "source": [
    "**Expected output:**\\\n",
    "0 <-> 1: 0.8591639399528503\\\n",
    "0 <-> 2: 0.777198314666748\\\n",
    "0 <-> 3: 0.7985224723815918\\\n",
    "0 <-> 4: 0.7754684090614319\\\n",
    "0 <-> 5: 0.8052163124084473\\\n",
    "1 <-> 2: 0.876341700553894\\\n",
    "1 <-> 3: 0.8321619629859924\\\n",
    "1 <-> 4: 0.823844850063324\\\n",
    "1 <-> 5: 0.8492751717567444\\\n",
    "2 <-> 3: 0.8241377472877502\\\n",
    "2 <-> 4: 0.8598626852035522\\\n",
    "2 <-> 5: 0.8579834699630737\\\n",
    "3 <-> 4: 0.9018082618713379\\\n",
    "3 <-> 5: 0.929144024848938\\\n",
    "4 <-> 5: 0.9185266494750977\n",
    "\n",
    "---\n",
    "You can print out the two sentences to see if the similarity score makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1BhZGMKc8t74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.\n",
      "We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "print(sentences[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngfL2vK38t75"
   },
   "source": [
    "---\n",
    "\n",
    "### 2.5 Play with summarization\n",
    "\n",
    "Lastly, let's play with the summarization pipelien provided by transformers. Be patient when the model is downloading. \n",
    "\n",
    "You can try the following code with different input text or arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ro8Ay1688t75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41923f52c5e4e93bbd75c7676482a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matteo\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Matteo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1461a75949754c8eb063aed31811cb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25fa2c163364744a9c8eea315d6fa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a59c0d1bccb477ead83bbee4b29330f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0e7b4a75784514a30a857cae1e748b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The hotness of the sun and the coldness of outer space are inexhaustible thermodynamic resources for human beings . From a thermodynamic point of view, any energy conversion systems that receive energy from the sun or dissipate energy to the universe are heat engines with photons as the \"working fluid\"'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "print(summarizer(text, max_length=150, min_length=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
